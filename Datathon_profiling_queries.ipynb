{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LrsUGW7JBzVm",
        "LTwP8J6EEZj4"
      ],
      "authorship_tag": "ABX9TyM/vFV96Cc7xQlPYtWYCVae",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frCheval/DATATHON-CPE-LYON-2023/blob/main/Datathon_profiling_queries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration du Notebook"
      ],
      "metadata": {
        "id": "LrsUGW7JBzVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Apache Spark and the Archive Unleashed Toolkit (AUT)."
      ],
      "metadata": {
        "id": "qr8SzHlZCM25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to True if working in Amazon SageMaker\n",
        "SAGEMAKER = False"
      ],
      "metadata": {
        "id": "n0EQG6u9CBss"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "\n",
        "APPS_HOME      = os.getcwd() + \"/apps\"\n",
        "\n",
        "SPARK_VERSION  = \"3.0.0\"\n",
        "HADOOP_VERSION = \"2.7\"\n",
        "AUT_VERSION    = \"0.91.0\"\n",
        "JAVA_VERSION   = \"11\"\n",
        "\n",
        "SPARK_HADOOP_VERSION = \"spark-{}-bin-hadoop{}\".format(SPARK_VERSION, HADOOP_VERSION)\n",
        "\n",
        "if SAGEMAKER:\n",
        "    !sudo amazon-linux-extras install java-openjdk11 -y\n",
        "    os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-11-openjdk-11.0.16.0.8-1.amzn2.0.1.x86_64\"\n",
        "else:\n",
        "    !apt-get install openjdk-\"$JAVA_VERSION\"-jdk-headless\n",
        "    os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-{}-openjdk-amd64\".format(JAVA_VERSION)\n",
        "\n",
        "!pip install -q findspark\n",
        "\n",
        "!wget https://archive.apache.org/dist/spark/spark-\"$SPARK_VERSION\"/\"$SPARK_HADOOP_VERSION\".tgz\n",
        "!wget https://github.com/archivesunleashed/aut/releases/download/aut-\"$AUT_VERSION\"/aut-\"$AUT_VERSION\".zip\n",
        "!wget https://github.com/archivesunleashed/aut/releases/download/aut-\"$AUT_VERSION\"/aut-\"$AUT_VERSION\"-fatjar.jar\n",
        "\n",
        "!tar -xf \"$SPARK_HADOOP_VERSION\".tgz\n",
        "!mkdir -p \"$APPS_HOME\"\n",
        "!mv spark-* aut-* \"$APPS_HOME\"\n",
        "\n",
        "!rm -rf sample_data \"$APPS_HOME\"/\"$SPARK_HADOOP_VERSION\".tgz"
      ],
      "metadata": {
        "id": "qIV9-RzMCFep"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark init"
      ],
      "metadata": {
        "id": "LTwP8J6EEZj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize spark in single-node cluster and configure pyspark with the AUT toolkit."
      ],
      "metadata": {
        "id": "_Drxs9BbEmUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "SPARK_DRIVER_MEMORY   = \"8g\"\n",
        "\n",
        "os.environ[\"SPARK_HOME\"] = \"{}/{}\".format(APPS_HOME, SPARK_HADOOP_VERSION)\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-memory {0} --jars {2}/aut-{1}-fatjar.jar --py-files {2}/aut-{1}.zip pyspark-shell'.format(SPARK_DRIVER_MEMORY, AUT_VERSION, APPS_HOME)\n",
        "\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "fmDP2nbREheB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import desc, col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "sc = pyspark.SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "0fF3LauYEw_1",
        "outputId": "c33b802d-652e-4390-9b5b-aefca02605e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://aacfaca14476:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Téléchargement du dataset"
      ],
      "metadata": {
        "id": "mXrUYo22KfGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "DIR=\"LIFRANUM\"\n",
        "!mkdir -p $DIR\n",
        "\n",
        "#!gsutil -m cp -r gs://cpe-lyon/LIFRANUM/autre $DIR\n",
        "#!gsutil -m cp -r gs://cpe-lyon/LIFRANUM/cartoweb $DIR\n",
        "#!gsutil -m cp -r gs://cpe-lyon/LIFRANUM/lifranum-method $DIR\n",
        "!gsutil -m cp -r gs://cpe-lyon/LIFRANUM/repo-ecritures-num $DIR"
      ],
      "metadata": {
        "id": "nEPUHNlnR6KK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyse du dataset"
      ],
      "metadata": {
        "id": "rlihy-vLSn5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Searching, ranking and grouping “Domains” within the data collections"
      ],
      "metadata": {
        "id": "6_OS8bfgSsF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from aut import *\n",
        "\n",
        "WARCs_path = \"LIFRANUM/repo-ecritures-num/*.warc*\""
      ],
      "metadata": {
        "id": "t4qxI_jrYETD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tldextract"
      ],
      "metadata": {
        "id": "P1l4x3pnTGfY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tldextract\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "@udf(\"string\")\n",
        "def extract_tld(s):\n",
        "    return tldextract.extract(s).suffix\n",
        "\n",
        "\n",
        "# Number of domain\n",
        "WebArchive(sc, sqlContext, WARCs_path) \\\n",
        "  .webpages() \\\n",
        "  .select(extract_tld(\"url\").alias(\"tld\")) \\\n",
        "  .groupBy(\"tld\") \\\n",
        "  .count() \\\n",
        "  .sort(desc(\"count\"))\\\n",
        "  .show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d62KjVn66M3i",
        "outputId": "62d02aeb-d65b-44b5-88e3-38f4025c29c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|tld |count|\n",
            "+----+-----+\n",
            "|com |7560 |\n",
            "|fr  |736  |\n",
            "|net |520  |\n",
            "|ca  |74   |\n",
            "|blog|22   |\n",
            "|org |6    |\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tldextract\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "@udf(\"string\")\n",
        "def extract_tld(s):\n",
        "    return tldextract.extract(s).suffix\n",
        "\n",
        "\n",
        "# % of domain\n",
        "WebArchive(sc, sqlContext, WARCs_path) \\\n",
        "  .webpages() \\\n",
        "  .select(extract_tld(\"url\").alias(\"tld\")) \\\n",
        "  .groupBy(\"tld\") \\\n",
        "  .count() \\\n",
        "  .sort(desc(\"count\"))\\\n",
        "  .show(10, False)"
      ],
      "metadata": {
        "id": "t3yTLF_Z69YJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}