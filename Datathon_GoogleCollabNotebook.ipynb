{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4lpX3RxISYwY",
        "ndMg7cPUWMOy",
        "zfb7i8qvxZOO",
        "wLfl84iK0jdG",
        "cGwXPm_BhTqW",
        "7ZpHqIW12k9J",
        "n_85Mpyfzlgt",
        "LnGWkIvRjd8M",
        "oCrPMFwzTOmW",
        "obsAx21wboYM",
        "OzT5ETtqTiQo",
        "E0u6_--fNvS4",
        "JouA5jflAccQ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frCheval/DATATHON-CPE-LYON-2023/blob/main/Datathon_GoogleCollabNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploration of WebArchives: Quickstart"
      ],
      "metadata": {
        "id": "n1OkFCDYoNgW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unYwCwjvg_Ct"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ-gJ-tsPg5z"
      },
      "source": [
        "Install [Apache Spark](https://spark.apache.org) and the [Archive Unleashed Toolkit](https://aut.docs.archivesunleashed.org/docs/home) (AUT).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to True if working in Amazon SageMaker\n",
        "SAGEMAKER = False"
      ],
      "metadata": {
        "id": "q-aJXsAugZEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F08YfGw-HBCw"
      },
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "\n",
        "APPS_HOME      = os.getcwd() + \"/apps\"\n",
        "\n",
        "SPARK_VERSION  = \"3.0.0\"\n",
        "HADOOP_VERSION = \"2.7\"\n",
        "AUT_VERSION    = \"0.91.0\"\n",
        "JAVA_VERSION   = \"11\"\n",
        "\n",
        "SPARK_HADOOP_VERSION = \"spark-{}-bin-hadoop{}\".format(SPARK_VERSION, HADOOP_VERSION)\n",
        "\n",
        "if SAGEMAKER:\n",
        "    !sudo amazon-linux-extras install java-openjdk11 -y\n",
        "    os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-11-openjdk-11.0.16.0.8-1.amzn2.0.1.x86_64\"\n",
        "else:\n",
        "    !apt-get install openjdk-\"$JAVA_VERSION\"-jdk-headless\n",
        "    os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-{}-openjdk-amd64\".format(JAVA_VERSION)\n",
        "\n",
        "!pip install -q findspark\n",
        "\n",
        "!wget https://archive.apache.org/dist/spark/spark-\"$SPARK_VERSION\"/\"$SPARK_HADOOP_VERSION\".tgz\n",
        "!wget https://github.com/archivesunleashed/aut/releases/download/aut-\"$AUT_VERSION\"/aut-\"$AUT_VERSION\".zip\n",
        "!wget https://github.com/archivesunleashed/aut/releases/download/aut-\"$AUT_VERSION\"/aut-\"$AUT_VERSION\"-fatjar.jar\n",
        "\n",
        "!tar -xf \"$SPARK_HADOOP_VERSION\".tgz\n",
        "!mkdir -p \"$APPS_HOME\"\n",
        "!mv spark-* aut-* \"$APPS_HOME\"\n",
        "\n",
        "!rm -rf sample_data \"$APPS_HOME\"/\"$SPARK_HADOOP_VERSION\".tgz\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark init"
      ],
      "metadata": {
        "id": "4lpX3RxISYwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize spark in [single-node cluster](https://docs.databricks.com/clusters/single-node.html) and configure pyspark with the AUT toolkit."
      ],
      "metadata": {
        "id": "QIVXv8GEquwZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge5UOwQJIDjs"
      },
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "SPARK_DRIVER_MEMORY   = \"8g\"\n",
        "\n",
        "os.environ[\"SPARK_HOME\"] = \"{}/{}\".format(APPS_HOME, SPARK_HADOOP_VERSION)\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-memory {0} --jars {2}/aut-{1}-fatjar.jar --py-files {2}/aut-{1}.zip pyspark-shell'.format(SPARK_DRIVER_MEMORY, AUT_VERSION, APPS_HOME)\n",
        "\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb-ED0zhZyGt"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import desc, col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "sc = pyspark.SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "sc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Web Archives"
      ],
      "metadata": {
        "id": "ndMg7cPUWMOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of [web archiving using WGET](https://wiki.archiveteam.org/index.php/Wget_with_WARC_output) as web crawler."
      ],
      "metadata": {
        "id": "XLKHVGdBrVei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile input.txt\n",
        "http://www.espinosa-oviedo.com\n",
        "http://www.vargas-solar.com"
      ],
      "metadata": {
        "id": "BCD8rDmBW80O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEVEL=1       # maximum number of links to follow (i.e, crawl depth)\n",
        "WAIT=0.1      # num. seconds to wait between consecutive calls\n",
        "\n",
        "IN_FILE       = \"input.txt\"  # list of URLs to crawl\n",
        "OUT_DIR       = \"WARC\"       # folder where crawl results will be stored\n",
        "OUT_WARC_FILE = \"out\"        # prefix for WARC files\n",
        "OUT_LOG_FILE  = \"log.txt\"    # file containing WGET log\n",
        "\n",
        "# https://www.gnu.org/software/wget/manual/wget.html\n",
        "!wget \\\n",
        "  --delete-after -nd \\\n",
        "  --input-file={IN_FILE}  \\\n",
        "  --level={LEVEL}    \\\n",
        "  --no-parent        \\\n",
        "  --wait={WAIT}      \\\n",
        "  --random-wait      \\\n",
        "  --adjust-extension \\\n",
        "  --reject=css,js,xml,rss,php  \\\n",
        "  --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15\" \\\n",
        "  --warc-file=out  \\\n",
        "  --warc-max-size=300m  \\\n",
        "  --no-warc-keep-log  \\\n",
        "  --output-file={OUT_LOG_FILE}\n"
      ],
      "metadata": {
        "id": "ZSCGG4i7WLzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move resulting files to the OUT_DIR folder\n",
        "!mkdir -p {OUT_DIR}\n",
        "!mv *.warc*  *.txt  {OUT_DIR}"
      ],
      "metadata": {
        "id": "7TXBnJDcXtvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying Web Archives"
      ],
      "metadata": {
        "id": "mOC7tF2wQVEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "\n",
        "* **AUT generates dataframes**. See the [AUT dataframe schemas](https://aut.docs.archivesunleashed.org/docs/dataframe-schemas) and the [Spark SQL guide](https://spark.apache.org/docs/3.0.0/sql-getting-started.html) for more info.\n",
        "* More examples are available in the [AUT documentation](https://aut.docs.archivesunleashed.org/docs/home).\n",
        "\n"
      ],
      "metadata": {
        "id": "VI5i2nTYxbb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from aut import *\n",
        "\n",
        "WARCs_path = \"WARC/*.warc*\""
      ],
      "metadata": {
        "id": "t4qxI_jrYETD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract webpages URLs"
      ],
      "metadata": {
        "id": "zfb7i8qvxZOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Dataframe API"
      ],
      "metadata": {
        "id": "durenU1UDgaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WebArchive(sc, sqlContext, WARCs_path) \\\n",
        "  .webpages() \\\n",
        "  .select(\"url\") \\\n",
        "  .show(20, False)"
      ],
      "metadata": {
        "id": "AApBC8RLGzcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark SQL equivalent"
      ],
      "metadata": {
        "id": "JuJGuReC-8MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = WebArchive(sc, sqlContext, WARCs_path).webpages()\n",
        "df.createOrReplaceTempView(\"webpages\")\n",
        "\n",
        "sql='''\n",
        "    SELECT url\n",
        "    FROM webpages\n",
        "'''\n",
        "\n",
        "sqlContext.sql(sql).show(20, False)"
      ],
      "metadata": {
        "id": "dHqQfKw-9po3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Top-Level Domains"
      ],
      "metadata": {
        "id": "wLfl84iK0jdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uses a [User Defined Function](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html) (UDF)"
      ],
      "metadata": {
        "id": "wHnHv-mFBqQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tldextract"
      ],
      "metadata": {
        "id": "MtQSGzj-A06z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tldextract\n",
        "tldextract.extract('http://forums.news.cnn.com/')    # See https://github.com/john-kurkowski/tldextract"
      ],
      "metadata": {
        "id": "LeAz-6zGBOq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Dataframe API"
      ],
      "metadata": {
        "id": "ojRT-AI3Dnix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tldextract\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "@udf(\"string\")\n",
        "def extract_tld(s):\n",
        "    return tldextract.extract(s).suffix\n",
        "\n",
        "WebArchive(sc, sqlContext, WARCs_path) \\\n",
        "  .webpages() \\\n",
        "  .select(extract_tld(\"url\").alias(\"tld\")) \\\n",
        "  .groupBy(\"tld\") \\\n",
        "  .count() \\\n",
        "  .sort(desc(\"count\"))\\\n",
        "  .show(10, False)"
      ],
      "metadata": {
        "id": "zGA2HW010mNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark SQL equivalent"
      ],
      "metadata": {
        "id": "7P6ctd2z_nUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = WebArchive(sc, sqlContext, WARCs_path).webpages()\n",
        "df.createOrReplaceTempView(\"webpages\")\n",
        "\n",
        "sqlContext.udf.register(\"extract_tld\", extract_tld)\n",
        "\n",
        "sql='''\n",
        "    SELECT tld, count(tld) AS count\n",
        "    FROM (\n",
        "      SELECT extract_tld(url) AS tld\n",
        "      FROM webpages\n",
        "    )\n",
        "    GROUP BY tld\n",
        "    ORDER BY count DESC\n",
        "'''\n",
        "\n",
        "sqlContext.sql(sql).show(20, False)"
      ],
      "metadata": {
        "id": "jIa09KKn_isx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count words in web pages\n",
        "\n",
        "Uses a [User Defined Function](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html) (UDF)"
      ],
      "metadata": {
        "id": "cGwXPm_BhTqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from aut import remove_html, remove_http_header\n",
        "from pyspark.sql.functions import col, udf\n",
        "\n",
        "@udf(\"Integer\")\n",
        "def word_count(s):\n",
        "  return len( s.split() )\n",
        "\n",
        "WebArchive(sc, sqlContext, WARCs_path) \\\n",
        "  .webpages()\\\n",
        "  .withColumn(\"text\", remove_html( remove_http_header(\"content\") ))\\\n",
        "  .withColumn(\"word_count\", word_count(\"text\"))\\\n",
        "  .select(\"text\", \"word_count\")\\\n",
        "  .show(1, False)"
      ],
      "metadata": {
        "id": "QapsdBfWiNPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark SQL equivalent"
      ],
      "metadata": {
        "id": "bb0XcphID2Es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = WebArchive(sc, sqlContext, WARCs_path)\\\n",
        "        .webpages()\\\n",
        "        .withColumn(\"text\", remove_html( remove_http_header(\"content\") ))   # AUT's remove_html & remove_http_header work only with dataframes\n",
        "\n",
        "df.createOrReplaceTempView(\"webpages_text\")\n",
        "\n",
        "@udf(\"Integer\")\n",
        "def word_count(s):\n",
        "  return len( s.split() )\n",
        "\n",
        "sqlContext.udf.register(\"word_count\", word_count)\n",
        "\n",
        "sql='''\n",
        "    SELECT text, word_count(text) AS word_count\n",
        "    FROM   webpages_text\n",
        "'''\n",
        "\n",
        "sqlContext.sql(sql).show(1, False)"
      ],
      "metadata": {
        "id": "GNzSOL7CD4cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count links between domains"
      ],
      "metadata": {
        "id": "7ZpHqIW12k9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from aut import extract_domain\n",
        "\n",
        "edges = WebArchive(sc, sqlContext, WARCs_path) \\\n",
        "  .webgraph()\\\n",
        "  .withColumn(\"src_domain\",  extract_domain(\"src\"))  \\\n",
        "  .withColumn(\"dest_domain\", extract_domain(\"dest\")) \\\n",
        "  .select([\"src_domain\", \"dest_domain\"])\\\n",
        "  .groupBy([\"src_domain\", \"dest_domain\"])\\\n",
        "  .count()\n",
        "\n",
        "edges.show(10, False)"
      ],
      "metadata": {
        "id": "LysFUZvk2n5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plots using [NetworkX](http://networkx.org) and [matplotlib](http:/:matplotlib.org)"
      ],
      "metadata": {
        "id": "UUBWxl34GBv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "df = edges.limit(10).toPandas()\n",
        "\n",
        "G = nx.from_pandas_edgelist(\n",
        "    df,\n",
        "    source=\"src_domain\",\n",
        "    target=\"dest_domain\",\n",
        "    edge_key=\"dest_domain\",\n",
        "    edge_attr=\"count\"\n",
        ")\n",
        "\n",
        "pos = nx.planar_layout(G)\n",
        "options = {\n",
        "    \"node_size\": 1000,\n",
        "    \"node_color\": \"#bc5090\",\n",
        "    \"node_shape\": \"o\",\n",
        "    \"alpha\": 0.5,\n",
        "    \"linewidths\": 4,\n",
        "    \"font_size\": 10,\n",
        "    \"font_color\": \"black\",\n",
        "    \"width\": 2,\n",
        "    \"edge_color\": \"grey\",\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "nx.draw(G, pos, with_labels=True, **options)\n",
        "labels = {e: G.edges[e][\"count\"] for e in G.edges}\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DnRVGKY05ps9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribution of HTTP Status Codes"
      ],
      "metadata": {
        "id": "n_85Mpyfzlgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "codes = WebArchive(sc, sqlContext, WARCs_path) \\\n",
        "  .all()\\\n",
        "  .groupBy('http_status_code')\\\n",
        "  .count()\n",
        "\n",
        "codes.show(20, True)"
      ],
      "metadata": {
        "id": "NHxYFefq92Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plots using [Plotly Express](https://plotly.com/python/plotly-express/)"
      ],
      "metadata": {
        "id": "gtYem_rCG3P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.bar(\n",
        "    codes.toPandas(),\n",
        "    x='http_status_code',\n",
        "    y='count'\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "kDGwQqAp79Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export graph to Gephi"
      ],
      "metadata": {
        "id": "LnGWkIvRjd8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See [Gephi Graph Viz Platform](http://gephi.org/)."
      ],
      "metadata": {
        "id": "CRsQ7c-WwSBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = WebArchive(sc, sqlContext, WARCs_path) \\\n",
        "          .webgraph() \\\n",
        "          .groupBy(\"crawl_date\", remove_prefix_www(extract_domain(\"src\")).alias(\"src_domain\"), remove_prefix_www(extract_domain(\"dest\")).alias(\"dest_domain\")) \\\n",
        "          .count() \\\n",
        "          .filter((col(\"dest_domain\").isNotNull()) & (col(\"dest_domain\") !=\"\")) \\\n",
        "          .filter((col(\"src_domain\").isNotNull()) & (col(\"src_domain\") !=\"\")) \\\n",
        "          .orderBy(desc(\"count\")) \\\n",
        "          .collect()\n",
        "\n",
        "WriteGEXF(graph, \"links-for-gephi.gexf\")"
      ],
      "metadata": {
        "id": "WwMaZdQnj3X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store results on disk"
      ],
      "metadata": {
        "id": "oCrPMFwzTOmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save as `csv` file with header"
      ],
      "metadata": {
        "id": "ypVqt2DpUppM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WebArchive(sc, sqlContext, WARCs_path) \\\n",
        "  .webgraph()\\\n",
        "  .limit(10)\\\n",
        "  .write.format('csv').save(\"webgraph\", header='true')"
      ],
      "metadata": {
        "id": "pEd3-cUITzbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save as `parquet` file (header automatically included)"
      ],
      "metadata": {
        "id": "dOCKywq5U9qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WebArchive(sc, sqlContext, WARCs_path) \\\n",
        "  .webgraph()\\\n",
        "  .limit(10)\\\n",
        "  .write.parquet(\"webgraph.parquet\")"
      ],
      "metadata": {
        "id": "qaHZe7AxVDuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read csv/parquet files"
      ],
      "metadata": {
        "id": "U25drGB0VXoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load parquet files\n",
        "df = sqlContext.read.parquet(\"webgraph.parquet\")\n",
        "df.show(2)\n",
        "df.printSchema()\n",
        "\n",
        "# load csv files\n",
        "df = sqlContext.read.option(\"header\", True).csv(\"webgraph\")\n",
        "df.show(2)\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "id": "ywF_O0N8VbpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extras"
      ],
      "metadata": {
        "id": "tYVHEPHkysOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collecting LIFRANUM Web Archives from Google Storage"
      ],
      "metadata": {
        "id": "rmGQc7fEWObC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "DIR=\"LIFRANUM\"\n",
        "!mkdir -p $DIR\n",
        "\n",
        "#!gsutil -m cp -r gs://cpe-lyon/LIFRANUM/autre $DIR\n",
        "#!gsutil -m cp -r gs://cpe-lyon/LIFRANUM/cartoweb $DIR\n",
        "#!gsutil -m cp -r gs://cpe-lyon/LIFRANUM/lifranum-method $DIR\n",
        "#!gsutil -m cp -r gs://cpe-lyon/LIFRANUM/repo-ecritures-num $DIR"
      ],
      "metadata": {
        "id": "AB0vmL5JbzJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accelerating operations"
      ],
      "metadata": {
        "id": "obsAx21wboYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caching dataframes in RAM accelerates spark operations (cf. [Spark DataFrame Cache and Persist Explained](https://sparkbyexamples.com/spark/spark-dataframe-cache-and-persist-explained))."
      ],
      "metadata": {
        "id": "p0Rbjch0b29x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WARCs_path = \"LIFRANUM/repo-ecritures-num/out-00000.warc.gz\"\n",
        "\n",
        "webpages = WebArchive(sc, sqlContext, WARCs_path).webpages()"
      ],
      "metadata": {
        "id": "PXlXxJV4bpON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without cache:"
      ],
      "metadata": {
        "id": "ZIKlQgZXcjHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "webpages.count()    # slow: spark loads the data, filter webpages, compute new columns, etc."
      ],
      "metadata": {
        "id": "hVD19YPbcDid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using cache"
      ],
      "metadata": {
        "id": "UoO1ZIzgcpvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "webpages.cache().count()    # first time slow because all previous operations are re executed"
      ],
      "metadata": {
        "id": "kqhwmQY5c0Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webpages.count()            # second time is faster"
      ],
      "metadata": {
        "id": "fXjsJ3a8c6Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading image from bytes"
      ],
      "metadata": {
        "id": "OzT5ETtqTiQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Reading image from string base64](https://dev.to/bl4ckst0n3/image-processing-how-to-read-image-from-string-in-python-pf8)\n",
        "\n"
      ],
      "metadata": {
        "id": "Knp3FfEBTkPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get an image's bytes from a WARC file\n",
        "res = WebArchive(sc, sqlContext, WARCs_path) \\\n",
        "  .images() \\\n",
        "  .select(\"bytes\")\\\n",
        "  .take(1)\n",
        "\n",
        "img_base64_string = res[0][0]"
      ],
      "metadata": {
        "id": "roSpmc_UUuI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "# load image from bytes\n",
        "decoded_string = io.BytesIO( base64.b64decode(img_base64_string) )\n",
        "Image.open(decoded_string)"
      ],
      "metadata": {
        "id": "uQiLofzKUNUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "E0u6_--fNvS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if SAGEMAKER:\n",
        "  !pip install -U pip setuptools wheel\n",
        "  !pip install -U spacy\n",
        "  !python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "q77t1ph56Cya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uses [Spacy.io](https://spacy.io/usage/linguistic-features#named-entities-101)"
      ],
      "metadata": {
        "id": "OVx-zhVKOrVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Recognizes english NERs\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "Jwyyx7baOzzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entity Name Recognition using a webpage in english"
      ],
      "metadata": {
        "id": "2KyyTf5UPWzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WebArchive(sc, sqlContext, WARCs_path) \\\n",
        "  .webpages() \\\n",
        "  .select(\"*\", remove_html(remove_http_header(\"content\")).alias(\"text\"))\\\n",
        "  .createOrReplaceTempView(\"webpages\")\n",
        "\n",
        "sql='''\n",
        "    SELECT language, text\n",
        "    FROM   webpages\n",
        "    WHERE  language=='en' AND text <> ''\n",
        "'''\n",
        "\n",
        "res = sqlContext.sql(sql).take(1)   # returns a list of 1 element\n",
        "txt = res[0][1]                     # \"text\" attribute from first element\n",
        "\n",
        "doc = nlp(txt)\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "_HmQGQH0PJso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating compressed Web Archives"
      ],
      "metadata": {
        "id": "JouA5jflAccQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upgrade WGET to produce compressed Web Archives (WARCs)."
      ],
      "metadata": {
        "id": "QMARIOqzqPpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "TMP_DIR = \"tmp\"\n",
        "!mkdir {TMP_DIR}\n",
        "%cd {TMP_DIR}\n",
        "\n",
        "!wget -nv http://ftp.gnu.org/gnu/wget/wget-1.21.tar.gz\n",
        "!tar -xzf wget-1.21.tar.gz\n",
        "!./wget-1.21/configure --with-ssl=openssl\n",
        "!sudo make\n",
        "!sudo make install\n",
        "\n",
        "%cd ..\n",
        "!sudo rm -r {TMP_DIR}"
      ],
      "metadata": {
        "id": "ZF3HMsmXSF_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile input.txt\n",
        "http://www.espinosa-oviedo.com\n",
        "http://www.vargas-solar.com"
      ],
      "metadata": {
        "id": "mcgoyhsmIbPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `--recursive` for crawling the totality of a website. See [WGET manual](https://www.gnu.org/software/wget/manual/wget.html)."
      ],
      "metadata": {
        "id": "Pj0XiIrEHS7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEVEL=1       # maximum number of links to follow (i.e, crawl depth)\n",
        "WAIT=0.1      # num. seconds to wait between consecutive calls\n",
        "\n",
        "IN_FILE       = \"input.txt\"  # list of URLs to crawl\n",
        "OUT_DIR       = \"WARC\"       # folder where crawl results will be stored\n",
        "OUT_WARC_FILE = \"out\"        # prefix for WARC files\n",
        "OUT_LOG_FILE  = \"log.txt\"    # file containing Wget's log\n",
        "\n",
        "!wget \\\n",
        "  --delete-after -nd \\\n",
        "  --input-file={IN_FILE}  \\\n",
        "  --level={LEVEL}     \\\n",
        "  --no-parent   \\\n",
        "  --wait={WAIT}    \\\n",
        "  --random-wait   \\\n",
        "  --adjust-extension \\\n",
        "  --reject=css,js,xml,rss,php  \\\n",
        "  --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15\" \\\n",
        "  --warc-file=out  \\\n",
        "  --warc-max-size=300m  \\\n",
        "  --no-warc-keep-log  \\\n",
        "  --output-file={OUT_LOG_FILE}\n"
      ],
      "metadata": {
        "id": "DvDUEhxlE6zl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}